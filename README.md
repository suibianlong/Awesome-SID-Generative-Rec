# Awesome Modular Semantic-based Generative Recommendation

> A curated list of resources based on the **Five-Stage Modular Analysis Framework**:
> **Representation** $\rightarrow$ **Tokenization** $\rightarrow$ **Generative Backbone** $\rightarrow$ **Training Paradigm** $\rightarrow$ **Inference**.

## ğŸ“– Taxonomy Overview

This repository organizes Generative Recommendation (GenRec) research not by listing papers chronologically, but by dissecting them into the **modular pipeline**. This perspective reveals how different methods innovate at specific stages of the generation process.

---

## 1. Representation Layer
*Capture the input semantics before discrete quantization.*

| Strategy | Description | Representative Papers |
| :--- | :--- | :--- |
| **Semantic Embedding** | Utilizing PLMs (BERT/ViT) to extract textual or visual features. | **TIGER** (NeurIPS'23), **LETTER** (arXiv'24) |
| **Collaborative / Graph** | Fusing interaction signals (CF) into the semantic space. | **EAGER** (arXiv'23), **LC-Rec** (WWW'24) |
| **Multimodal Unified** | Jointly modeling text, image, and ID features. | **VGA** (ACL'24) |
| **Continuous Interaction** | Directly using raw interaction vectors (for Diffusion models). | **DiffRec** (SIGIR'23), **DDRM** (SIGIR'24) |

| Strategy Family | Sub-Category | Paper's Tokenization & Details |
| :--- | :--- | :--- |
| **Semantic Embedding** | | **TIGER** <br>  <br> *ä½¿ç”¨è¯­ä¹‰ä¿¡æ¯è¡¨ç¤ºitem* |
| **Semantic Embedding** | **Multi-source Semantic** | **GREAM** <br>  <br> *ä½¿ç”¨å¤šæºä¿¡å·ï¼ˆåŒ…æ‹¬ç‰©å“çš„æ ‡é¢˜ã€å®˜æ–¹æè¿°å’Œé«˜è´¨é‡ç”¨æˆ·è¯„è®ºï¼‰ï¼Œå¹¶ä½¿ç”¨LLMå¯¹è¿™äº›ä¿¡æ¯è¿›è¡Œé‡å†™ï¼Œå®ç°å…¨é¢ã€ç‰¹å¾ä¸°å¯Œçš„ç‰©å“æè¿°ï¼Œæœ€åå°†é‡å†™åçš„ä¿¡æ¯ä½¿ç”¨LLMç¼–ç å½¢æˆåµŒå…¥å‘é‡* |
| **Collaborative / Graph** | **LLMs** | **LC-Rec** <br>  <br> *ä½¿ç”¨LLMç”Ÿæˆæœ€åˆçš„ç‰©å“è¡¨ç¤ºï¼Œä»–æ‰€è½¬æ¢çš„ä¿¡æ¯ä¸ºäº¤äº’ä¿¡æ¯ï¼Œå¹¶éå•çº¯çš„æ–‡æœ¬ä¿¡æ¯*  |
| **Collaborative / Graph** | **Dual Codes** | **EAGER** (arXiv'23) <br> [EAGERåŒæµç»“æ„å›¾] <br> *åŒæ—¶æ¥å—è¡Œä¸ºåŠè¯­ä¹‰ä¿¡æ¯ï¼Œä¸¤éƒ¨åˆ†åˆ†åˆ«é€šè¿‡ä¸åŒçš„é¢„è®­ç»ƒæ¥ç”Ÿæˆembedding representationï¼Œå¹¶åˆ†åˆ«å°†å…¶ä¼ ç»™åé¢çš„tokenizerå¤„ç†ã€‚*|

---

## 2. Tokenization Layer: Discretization for Generation
*å°†è¿ç»­è¡¨å¾ç¦»æ•£åŒ–ä¸ºå¯ç”Ÿæˆ Token (Codebooks)ã€‚*

| Tokenizer Family | Sub-Category | Paper's Tokenization Focus & Details |
| :--- | :--- | :--- |
| **Residual Quantization (RQ)** | **RQ-VAE** | **TIGER** (NeurIPS'23) <br> <img src="./assets/Tokenization-TIGER.png" width="600" /> <br> *åˆ©ç”¨å¤šå±‚æ®‹å·®é‡åŒ–å™¨å°† Item Embedding ç¼–ç ä¸ºå›ºå®šé•¿åº¦çš„ Token åºåˆ—ï¼Œé¦–æ¬¡å®ç° ID åˆ° Token çš„è½¬æ¢ã€‚* |
| | **RQ-VAE** | **LC-Rec** <br>  <br> *ä½¿ç”¨RQ-VAEå®ç°ä»embeddingåˆ°item IDçš„è½¬æ¢ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è§£å†³è¯­ä¹‰æ˜ å°„å†²çªçš„æ–¹å¼ï¼Œå¼•å…¥uniform distribution constraintï¼Œå°†è¿™ä¸ªå‡åŒ€è¯­ä¹‰æ˜ å°„çš„ä»»åŠ¡è½¬æ¢ä¸ºoptimal transmission problemï¼Œå¹¶é€šè¿‡Sinkhorn-Knopp algorithmè§£å†³ã€‚* |
| | **RQ-VAE** | **MQL4GRec** <br>  <br> *æ¯ä¸ªæ¨¡æ€ï¼ˆæ–‡æœ¬ã€å›¾åƒï¼‰è®­ç»ƒä¸€ä¸ªç‹¬ç«‹çš„RQ-VAEç¿»è¯‘å™¨ï¼Œå®ç°ä¸åŒæ¨¡æ€çš„ä¿¡å·åœ¨è¾“å‡ºç©ºé—´çš„è¯­ä¹‰ä¸Šå¯¹é½* |
| | **RQ-VAE** | **LLaDA-Rec** <br>  <br> *æå‡º Multi-Head VQ-VAE æ¥ç”Ÿæˆå¹¶è¡Œçš„è¯­ä¹‰IDã€‚å…·ä½“åšæ³•æ˜¯ï¼šå°†ç‰©å“çš„è¯­ä¹‰å‘é‡åˆ†å‰²æˆMä¸ªå­å‘é‡ï¼Œæ¯ä¸ªå­å‘é‡åˆ†åˆ«ä½¿ç”¨ä¸€ä¸ªç‹¬ç«‹çš„ç æœ¬è¿›è¡Œé‡åŒ–* |
| |  | **LETTER** (CIKM'24) <br> <img src="./assets/Tokenization-LETTER.png" width="600" /> <br> *æå‡ºäº†å¯å­¦ä¹ çš„ Tokenizerï¼Œé€šè¿‡ **RQ-VAE** (è¯­ä¹‰æ­£åˆ™åŒ–)ã€**å¯¹æ¯”å¯¹é½æŸå¤±** (ååŒæ­£åˆ™åŒ–) å’Œ **å¤šæ ·æ€§æŸå¤±** å…±åŒä¼˜åŒ–ä»£ç æœ¬ï¼Œè§£å†³äº†ç°æœ‰ä»£ç æœ¬ç¼ºä¹ååŒä¿¡å·å’Œåˆ†é…åå·®çš„é—®é¢˜ã€‚* |
| | **R-KMeans** | OneRec  |
| **Product Quantization (PQ)** | **PQ** | RPG |
| | **OPQ** | **RPG** (KDD'25) <br> [RPGå¹¶è¡Œç”Ÿæˆ/OPQç¤ºæ„å›¾] <br> *è´¡çŒ®: é‡‡ç”¨äº†ç±»ä¼¼ OPQ çš„ç»“æ„æ¥æ„å»º Long Semantic IDï¼Œä»¥æ”¯æŒéè‡ªå›å½’çš„å¹¶è¡Œç”Ÿæˆï¼Œè§£å†³äº†çŸ­ ID è¯­ä¹‰å®¹é‡ä¸è¶³çš„é—®é¢˜ã€‚* |
| **Clustering-based** | **Hierarchical K-Means** | **GenRet** (NeurIPS'22) <br> [GenRetæ ‘ç»“æ„IDç”Ÿæˆå›¾] <br> *è´¡çŒ®: å°† Item ID ç¼–ç æˆæ ‘è·¯å¾„åºåˆ—ï¼Œåˆ©ç”¨å±‚æ¬¡ K-Means èšç±»æ„å»ºæ ‘ç»“æ„ï¼Œå°†ç”Ÿæˆé—®é¢˜è½¬åŒ–ä¸ºè·¯å¾„ç”Ÿæˆã€‚* |
| | **Hierarchical K-Means** | **SEER** (RecSys'23) <br> [SEER IDè§£é‡Šæ€§ç»“æ„å›¾] <br> *è´¡çŒ®: é‡‡ç”¨å±‚æ¬¡åŒ–çš„ç¦»æ•£ ID ç»“æ„ï¼Œåˆ©ç”¨èšç±»ç»“æœæ¥æä¾›æ¨èçš„å¯è§£é‡Šæ€§ï¼Œå¹¶æŒ‡å¯¼ç”Ÿæˆè¿‡ç¨‹ã€‚* |
| | **Hierarchical K-Means** | **ColaRec** <br> <br> *è´¡çŒ®: ä½¿ç”¨å±‚æ¬¡åŒ–K-meanså¯¹CFä¿¡å·æ„å»ºå¯¹åº”çš„GID* |
| **Hybrid / Textual** | **Raw Text Tokens** | **GPT4Rec** (SIGIR eCom'23) <br> [GPT4Rec Query/Textç”Ÿæˆæµç¨‹å›¾] <br> *è´¡çŒ®: å°†ç”¨æˆ·å†å²è½¬åŒ–ä¸ºæ–‡æœ¬ Queryï¼Œç„¶åç”Ÿæˆ Item Title ç­‰æè¿°æ€§æ–‡æœ¬ï¼Œå®Œå…¨ç»•è¿‡äº† Item ID è¯­ä¹‰é‡åŒ–æ­¥éª¤ã€‚* |
| | **ID + Text Mixing** | **OneRec** (arXiv'25) <br> [OneRecæ··åˆTokenè¾“å…¥ç¤ºæ„å›¾] <br> *è´¡çŒ®: å°†ç¦»æ•£çš„ Item ID Token å’Œè¿ç»­çš„æ–‡æœ¬ Token ä½œä¸º LLM çš„è¾“å…¥ï¼Œå®ç°äº†ç»Ÿä¸€çš„æ£€ç´¢ä¸æ’åºã€‚* |
| **Learnable / E2E** | **Joint Optimization** | **LC-Rec** (WWW'24) <br> [LC-Recè”åˆä¼˜åŒ–æ¡†å›¾] <br> *è´¡çŒ®: æå‡ºäº†å¯å­¦ä¹ çš„ä»£ç æœ¬ï¼Œè®©é‡åŒ–å™¨åœ¨æ¨èä»»åŠ¡ä¸­åŒæ­¥ä¼˜åŒ–ï¼Œä»¥é€‚é…ç”Ÿæˆå¼éª¨å¹²ã€‚* |
| | **Joint Optimization** | **ETEGRec** (CIKM'24) <br> [ETEGRecç«¯åˆ°ç«¯æ¶æ„å›¾] <br> *è´¡çŒ®: ä¾§é‡äºå®ç° Tokenizer ä¸ç”Ÿæˆæ¨¡å—çš„ç«¯åˆ°ç«¯å¯è®­ç»ƒæ€§ï¼Œå‡å°‘é‡åŒ–è¯¯å·®å¯¹æ¨èæ€§èƒ½çš„å½±å“ã€‚* |

---

## 3. Generative Backbone
*The architecture modeling the probability of the token sequence.*

| Architecture | Pros & Cons | Representative Papers |
| :--- | :--- | :--- |
| **Encoder-Decoder (T5/BART)** | Bi-directional context encoding; good for mapping History $\to$ Target. | **TIGER**, **P5**, **VQ-Rec** |
| **Decoder-Only (LLM/GPT)** | Strong reasoning & zero-shot ability; standard for LLM-based approaches. | **OneRec**, **GPT4Rec**, **SGL** |
| **Non-Autoregressive (NAR)** | Parallel generation; significantly faster inference but harder to train. | **RPG** (KDD'25) |
| **Diffusion (Denoising)** | Iterative noise removal; generates continuous vectors, not tokens. | **DiffRec**, **LDiffRec** |

| Architecture Family | Sub-Category | Paper's Backbone Focus & Details |
| :--- | :--- | :--- |
| **Encoder-Decoder (T5/BART)** | **Transformer** | **TIGER** <br>  <br> *é‡‡ç”¨æ ‡å‡†çš„ Transformer ç¼–ç å™¨-è§£ç å™¨ æ¶æ„ä½œä¸ºåºåˆ—åˆ°åºåˆ—ç”Ÿæˆæ¨¡å‹* |
| **Encoder-Decoder** | **Two-Stream Generation Architecture**| **EAGER** (arXiv'23) <br> [åŒæµæ¶æ„å›¾] <br> *ä½¿ç”¨ä¸€ä¸ªå…±äº«ç¼–ç å™¨å’Œä¸¤ä¸ªè§£ç å™¨ï¼Œä¸¤ä¸ªè§£ç å™¨åˆ†åˆ«å¯¹åº”ä¸€ç§åºåˆ—ä¿¡æ¯ï¼Œå®ç°äº†åŒæ—¶åŸºäºä¸¤ç§åºåˆ—è¿›è¡Œé¢„æµ‹*  |
| **Only Encoder** | **VQ-Rec**| **VQ-Rec** <br>  <br> *å°†tokenizerç”Ÿæˆçš„ç¦»æ•£codeåœ¨è¾“å…¥transformerå‰åŠ äº†ä¸€æ­¥Code Embedding Lookupï¼Œæ—¶æ¨¡å‹å¯ä»¥é€šè¿‡å¾®è°ƒï¼Œæ”¹å˜å¯¹åº”çš„æŸ¥æ‰¾çŸ©é˜µï¼Œä»¥é€‚åº”ä¸åŒçš„ä¸‹æ¸¸ä»»åŠ¡*  |
| **Decoder-Only (LLM/GPT)** | **LLMs**| **LC-Rec** <br>  <br> *æ¨¡å‹ç°åœ¨ä»¥Sequential Item Predictionä¸ºæ ¸å¿ƒä»»åŠ¡ï¼Œä½¿ç”¨ Explicit Index-Language Alignmentå’Œ Implicit Recommendation-oriented Alignmentå¯¹åŸæœ¬çš„LLMè¿›è¡Œè°ƒæ•´ï¼Œä½¿å…¶èƒ½å¤Ÿé€šè¿‡item IDæ›´å‡†ç¡®çš„é¢„æµ‹ç»“æœ* |
|| **Two-Stream Generation Architecture**| **EAGER** (arXiv'23) <br> [åŒæµæ¶æ„å›¾] <br> *ä½¿ç”¨ä¸€ä¸ªå…±äº«ç¼–ç å™¨å’Œä¸¤ä¸ªè§£ç å™¨ï¼Œä¸¤ä¸ªè§£ç å™¨åˆ†åˆ«å¯¹åº”ä¸€ç§åºåˆ—ä¿¡æ¯ï¼Œå®ç°äº†åŒæ—¶åŸºäºä¸¤ç§åºåˆ—è¿›è¡Œé¢„æµ‹*  |
| **Decoder-Only (LLM/GPT)** | Strong reasoning & zero-shot ability; standard for LLM-based approaches. | **OneRec**, **GPT4Rec**, **SGL** |
| **Non-Autoregressive (NAR)** |  | **RPG** <br>  <br> *1.åœ¨åºåˆ—ç¼–ç å™¨ï¼ˆTransformerï¼‰çš„æœ€ç»ˆè¾“å‡ºä¹‹åï¼Œè¿æ¥äº† $m$ ä¸ªç‹¬ç«‹çš„ã€å¹¶è¡Œçš„é¢„æµ‹å¤´ï¼ˆMLPæŠ•å½±å¤´ï¼‰ï¼Œæ¯ä¸ªå¤´ $g_j(Â·)$ è´Ÿè´£é¢„æµ‹è¯­ä¹‰IDä¸­ç¬¬ $j$ ä¸ªä½ç½®ä¸Šçš„tokenã€‚2.å¼•å…¥äº†ä¸€ä¸ªç‰©å“çº§åˆ«çš„èšåˆè¡¨ç¤ºå±‚ã€‚ä»–ä»¬å…ˆå°†ä¸€ä¸ªç‰©å“çš„æ‰€æœ‰tokenåµŒå…¥è¿›è¡Œèšåˆï¼Œå¾—åˆ°ä¸€ä¸ªå•ä¸€çš„ç‰©å“å‘é‡ $v_i$ï¼Œå†å°†è¿™ä¸ªç‰©å“å‘é‡ä½œä¸ºåºåˆ—æ¨¡å‹çš„è¾“å…¥*  |
| **Non-Autoregressive (NAR)** |  | **LLaDA-Rec** <br>  <br> *é‡‡ç”¨åŸºäºåŒå‘Transformerçš„ç¦»æ•£æ‰©æ•£æ¨¡å‹ä½œä¸ºç”Ÿæˆæ¨¡å‹çš„æ ¸å¿ƒ*  |

---

## 4. Training Paradigm
*How the system is optimized and aligned.*

| Paradigm | Description | Representative Papers |
| :--- | :--- | :--- |
| **Two-Stage (Quantize $\to$ Train)** | Step 1: Train Codebook (VQ-VAE). Step 2: Train Generator (Seq2Seq). | **TIGER**, **VQ-Rec** |
| **Joint / End-to-End** | Optimizing quantization loss and generation loss simultaneously. | **LC-Rec**, **GeneRec** |
| **Pre-train & Fine-tune** | Standard LLM paradigm: Language Modeling pre-training $\to$ Rec fine-tuning. | **GPT4Rec**, **P5** |
| **Alignment (RLHF/DPO)** | Aligning generation with ranking metrics or user feedback (Reinforcement Learning). | **OneRec** (Preference Alignment), **TallRec** |

| Paradigm Family | Sub-Category | Paper's Paradigm Focus & Details |
| :--- | :--- | :--- |
| **Two-Stage (Quantize $\to$ Train)** | | **TIGER** <br> <br> *Step 1: Train Codebook (VQ-VAE). Step 2: Train Generator (Seq2Seq).* |
| **Two-Stage (Quantize $\to$ Train)** | **Multi-Task Enhancement** | **EAGER** (arXiv'23) <br> [åŒæµæ¶æ„å›¾] <br> *æ·»åŠ äº†ä¸€ä¸ªGlobal Ctracive Taskå’Œä¸€ä¸ªSemantic-guided transformer Taskï¼Œå‰ä¸€ä¸ªåŠ å¼ºäº†æ¨¡å‹å¯¹tokenså…¨å±€çš„åˆ¤åˆ«èƒ½åŠ›ï¼Œåè€…ä½¿ç”±åŒæµæ¨¡å‹çš„ä¸¤ä¸ªæµä¹‹é—´ä¹‹é—´äº§ç”Ÿä¿¡æ¯äº¤æ¢ï¼Œè¿™ä¸¤è€…éƒ½æ˜¯è¾…åŠ©è®­ç»ƒç›®æ ‡* |
| **Two-Stage (Quantize $\to$ Train)** | **Multi-Task Enhancement** | **ColaRec**  <br> <br> *å¼•å…¥ Item-Item Indexingå’Œcontrastive lossï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿç†è§£ä»itemåˆ°GIDçš„æ˜ å°„åŠæ‹‰è¿‘ç›¸ä¼¼GIDåœ¨è¯­ä¹‰åµŒå…¥ä¸Šçš„è·ç¦»ï¼Œå®ç°è¯­ä¹‰å’ŒCFä¿¡å·çš„å¯¹å…¶æœºåˆ¶ã€‚* |
| **Two-Stage (Quantize $\to$ Train)** | **Mix of Generative and Dense Retrieval** | **LIGER**  <br> <br> *ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦æŸå¤±ä¸the next-token prediction losså½¢æˆåŒç›®æ ‡æŸå¤±ï¼Œå®ç°ç¨ å¯†æ£€ç´¢ä¸ç”Ÿæˆå¼æ£€ç´¢ç»“åˆçš„æ•ˆæœ* |
| **Two-Stage (Quantize $\to$ Train)** | **MTP** | **RPG** <br> <br> *å¼•å…¥å¹¶é€‚åº”äº†â€œå¤štokené¢„æµ‹â€ç›®æ ‡ã€‚è¯¥ç›®æ ‡è®©æ¨¡å‹åŒæ—¶é¢„æµ‹è¯­ä¹‰IDçš„æ‰€æœ‰tokenï¼Œä»è€Œæ‰“ç ´äº†tokené—´çš„é¡ºåºä¾èµ–ï¼Œå®ç°å¹¶è¡Œè§£ç ã€‚* |
| **Two-Stage (Quantize $\to$ Train)** | **åŒå±‚æ¬¡æ©ç è®­ç»ƒ** | **LLaDA-Rec** <br> <br> *ä½¿ç”¨User-History Level Maskingï¼Œå®ç°ç‰©å“é—´çš„åºåˆ—ä¾èµ–å…³ç³»çš„å­¦ä¹ ï¼Œä½¿ç”¨Next-Item Level Maskingï¼Œå®ç°å‹ç‰©å“å†…çš„è¯­ä¹‰ç»“æ„çš„å­¦ä¹ * |
| **Joint / End-to-End** | **alignment tuning** | **LC-Rec** <br>  <br> *æ¨¡å‹ç°åœ¨ä»¥Sequential Item Predictionä¸ºæ ¸å¿ƒä»»åŠ¡ï¼Œä½¿ç”¨ Explicit Index-Language Alignmentå’Œ Implicit Recommendation-oriented Alignmentå¯¹åŸæœ¬çš„LLMè¿›è¡Œè°ƒæ•´ï¼Œå®ç°llmä¸item IDçš„å¯¹é½* |
| **Joint / End-to-End** | **Quantizeä¸TrainååŒ** | **ETEGRec** <br>  <br> *å°†item tokenizationå’Œgenerative recommendationåœ¨ä¸€ä¸ªæ¡†æ¶ä¸­è”åˆä¼˜åŒ–ï¼Œå¹¶æå‡ºSequence-Item Alignmentå’ŒPreference-Semantic Alignmentå¯¹é½ç­–ç•¥ï¼Œå®ç°åºåˆ—åˆ°ç‰©å“åˆ†å¸ƒä»¥åŠåå¥½åˆ°è¯­ä¹‰çš„å¯¹é½ï¼›åŒæ—¶æå‡ºäº¤æ›¿ä¼˜åŒ–ç­–ç•¥ï¼Œè§£å†³ç«¯åˆ°ç«¯è®­ç»ƒçš„ä¸ç¨³å®šæ€§* |
| **Joint / End-to-End** | **ä»¥end-to-endä¸ºéª¨æ¶ï¼ŒåµŒå…¥alignment** | **GREAM** <br>  <br> *åˆ›å»ºå¤šå±‚æ¬¡å¯¹é½ä»»åŠ¡ï¼ŒåŒ…æ‹¬é¡ºåºæ¨èä»»åŠ¡ï¼Œè¯­ä¹‰é‡å»ºä»»åŠ¡åŠç”¨æˆ·åå¥½ä»»åŠ¡ï¼Œå®ç°åœ¨LLMçš„è¡¨ç¤ºç©ºé—´ä¸­ï¼Œå»ºç«‹ä¸€ä¸ªåŒæ—¶æ‰¿è½½äº†è¯­è¨€è¯­ä¹‰å’Œåä½œè¯­ä¹‰çš„æ··åˆè¡¨ç¤ºï¼›åœ¨ä¼ ç»Ÿæ¨ç†é“¾ä¸ŠåŠ å…¥å»å™ªåºåˆ—é‡å†™ï¼Œå®ç°æ¨¡å‹ä¸»åŠ¨è¯†åˆ«å¹¶è¿‡æ»¤æ‰ç”¨æˆ·å†å²ä¸­çš„å™ªå£°äº¤äº’ï¼›ä½¿ç”¨åˆæˆCoTæ•°æ®ï¼Œåå‘æ¨ç†ç”Ÿæˆé«˜è´¨é‡æ•°æ®é›†ï¼Œå°†SFTä¸è¯¾ç¨‹å­¦ä¹ è°ƒåº¦ç›¸èåˆï¼›æå‡ºSRPO ç®—æ³•ï¼ˆåŒ…æ‹¬æ®‹å·®æ•æ„Ÿçš„å¯éªŒè¯å¥–åŠ±ï¼Œå¥–åŠ±æ ¡å‡†çš„ä¼˜åŠ¿ä¼°è®¡å’Œæ–¹å·®å¼•å¯¼çš„åŠ¨æ€é‡‡æ ·ï¼‰ï¼Œè§£å†³åº”ç”¨RLHFä¼šå› å¥–åŠ±ç¨€ç–å¯¼è‡´è®­ç»ƒä¸ç¨³å®šã€éš¾ä»¥æ”¶æ•›* |
| **Pre-train & Fine-tune** | | **VQ-Rec**  <br>  <br> *åœ¨ä¼ ç»Ÿçš„é¢„è®­ç»ƒ-å¾®è°ƒèŒƒå¼çš„åŸºç¡€ä¸Šï¼Œåœ¨é¢„è®­ç»ƒæ—¶ï¼Œä½¿ç”¨å¯¹æ¯”å­¦ä¹ ï¼Œä¿è¯æ¨¡å‹åœ¨å¤šé¢†åŸŸçš„é€‚åº”æ€§ï¼Œåœ¨å¾®è°ƒé˜¶æ®µæå‡ºpermutation-based alignmentå®Œæˆå¯¹code embedding tableå¯¹äºä¸åŒä¸‹æ¸¸ä»»åŠ¡çš„è°ƒæ•´* |
| **Pre-train & Fine-tune** | **Cross-modal** | **MQL4GRec**  <br>  <br> *å¼•å…¥Asymmetric Item Generation å’Œ Quantitative Language Alignment å®ç°æ¨¡æ€é—´çš„çŸ¥è¯†è¿ç§»ä¸å¯¹é½* |
| **Alignment (RLHF/DPO)** | **Alignment+fine-tune** | **OneRec**  <br>  <br> *ä½¿ç”¨èç³»ç»Ÿçš„æ•°æ®ï¼ˆç‰©å“çš„è¯­ä¹‰IDåºåˆ—ï¼‰å¯¹å…¶è¿›è¡Œç›‘ç£å¾®è°ƒï¼Œåœ¨ä½¿ç”¨GRPOæ¥å¯¹é½å’Œä¼˜åŒ–æ¨¡å‹çš„ç”Ÿæˆåå¥½ï¼ˆå…·ä½“æ–¹æ³•ä¸ºå…¨æµç¨‹SID-æ–‡æœ¬å¯¹é½å’Œæ’åºæ„ŸçŸ¥çš„å¥–åŠ±å‡½æ•°ï¼‰* |

---

## 5. Inference & Decoding
*Strategies to generate valid items and rank them efficiently.*

| Strategy | Description | Representative Papers |
| :--- | :--- | :--- |
| **Constrained Beam Search** | Using a Prefix Tree (Trie) to force the generator to output valid Item IDs. | **TIGER**, **GenRet**, **LETTER** |
| **Standard Beam Search** | Generating top-K sequences based on probability (may hallucinate invalid IDs). | **GPT4Rec** |
| **Parallel / Graph Decoding** | Non-autoregressive decoding guided by graph constraints. | **RPG** |
| **Re-ranking / Scoring** | Using the generator to score candidates retrieved by another model. | **LLaRA**, **TALLRec** |

| Family | Sub-Category | Details |
| :--- | :--- | :--- |
| **Constrained Beam Search** | **beam serch + Temperature-based sampling** | **TIGER** <br> <br> *ä½¿ç”¨ Beam Search è¿›è¡Œè‡ªå›å½’è§£ç ä»¥ç”Ÿæˆè¯­ä¹‰IDï¼Œå¹¶å¼•å…¥äº†æ¸©åº¦é‡‡æ ·æ¥æ§åˆ¶æ¨èå¤šæ ·æ€§* |
| **Constrained Beam Search** | **Conffdence-based Ranking** | **EAGER** (arXiv'23) <br> [åŒæµæ¶æ„å›¾] <br> *åœ¨åŒæµæ¨¡å‹çš„æ¯ä¸ªæµä¸­ä½¿ç”¨beam searchï¼Œåˆ†åˆ«é€‰å‡ºå„è‡ªçš„top-kï¼Œç„¶åå¯¹è¿™2kçš„é¢„æµ‹ç»“æœä¸­ï¼Œè®¡ç®—ä»–ä»¬çš„log probabilitiesï¼Œå€¼è¶Šå°ä»£è¡¨å¯èƒ½æ€§è¶Šå¤§ï¼Œå¯¹è¿™2kä¸ªç»“æœçš„åˆ†æ•°è¿›è¡Œæ’åºï¼Œå–top-kä¸ºç»“æœ* |
| | **beam serch** | **LC-Rec**  <br>  <br> *ä½¿ç”¨beam searchæ›¿æ¢LLMåŸæœ¬çš„å€™é€‰é›†inferenceæ–¹æ³•ï¼Œå¹¶ä¸”åˆ©ç”¨KV bufferåŠ é€Ÿè¿™ä¸€è¿‡ç¨‹ï¼Œå¹¶ä¸”åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­è¿‡æ»¤æ‰éæ³•ç´¢å¼•ç»„åˆã€‚* |
| | **é€‚ç”¨äºç¦»æ•£æ‰©æ•£** | **LC-Rec**  <br>  <br> *åŒ…å«ä¸‰æ­¥ï¼š1.è‡ªé€‚åº”ç”Ÿæˆé¡ºåºï¼šåœ¨æ¯ä¸€æ­¥ï¼Œé€‰æ‹©æ¨¡å‹ç½®ä¿¡åº¦æœ€é«˜ï¼ˆè€Œéå›ºå®šä½ç½®ï¼‰çš„tokenè¿›è¡Œç”Ÿæˆã€‚2.æŸæœç´¢é›†æˆï¼šåœ¨é€‰å®šçš„ä½ç½®ä¸Šè¿›è¡ŒæŸæ‰©å±•å’Œå‰ªæï¼Œä»¥ç”Ÿæˆtop-kæ¨èç»“æœã€‚3.è¿­ä»£ç²¾ç‚¼ï¼šç”ŸæˆæŸäº›ä½ç½®åï¼Œå¯¹å‰©ä½™ä½ç½®ä¿¡åº¦çš„ä½ç½®è¿›è¡Œé‡æ–°æ©ç ï¼Œåœ¨åç»­æ­¥éª¤ä¸­åˆ©ç”¨æ–°çš„ä¸Šä¸‹æ–‡ä¿¡æ¯é‡æ–°é¢„æµ‹ã€‚* |
| **Standard Beam Search** | **Multi - mode** | **GREAM** <br>  <br> *æœ‰ä¸¤ç§æ¨èæ¨¡å¼ï¼Œä¸€ç§åŸºäºCF signals,å¦ä¸€ç§å…ˆåŸºäºé€»è¾‘ç”Ÿæˆå†åŸºäºCFä½†æ˜¯ä¸¤æ­¥ç­›é€‰éƒ½åŸºäºbeam search* |
| **Parallel / Graph Decoding** | | **RPG**<br>  <br> *ä¸ºæ‰€æœ‰æœ‰æ•ˆçš„è¯­ä¹‰IDæ„å»ºä¸€ä¸ªå›¾ç»“æ„ï¼Œå¹¶é€šè¿‡è¿­ä»£çš„å›¾ä¼ æ’­æ¥å¼•å¯¼è§£ç è¿‡ç¨‹ï¼Œå®ç°äº†åœ¨å·¨å¤§çš„å€™é€‰ç©ºé—´ä¸­æ‰¾åˆ°é«˜åˆ†çš„ã€æœ‰æ•ˆçš„è¯­ä¹‰IDï¼Œè€Œä¸éœ€è¦æšä¸¾æ‰€æœ‰ç‰©å“ï¼Œæœ¬è´¨æ˜¯å…ˆé€šè¿‡ä¹‹å‰æ„å»ºçš„è§£ç å›¾æ‰¾åˆ°é«˜åˆ†å€™é€‰çš„ç›¸ä¼¼è¯­ä¹‰IDï¼Œå†è®¡ç®—scoreå¾—åˆ°æ–°çš„é«˜åˆ†å€™é€‰ï¼Œç”±æ­¤è¿­ä»£qæ¬¡ï¼Œæœ€ç»ˆæ‰¾åˆ°æœ€ä¼˜* |
| **Re-ranking / Scoring** | **beam serch+scoring+ranking** |  **LIGER** <br> <br> *inferenceåˆ†ä¸ºä¸¤æ­¥ï¼Œç¬¬ä¸€æ­¥ä½¿ç”¨beam searchå¾—åˆ°ä¸€ä¸ªåˆå§‹å€™é€‰åˆ—è¡¨ï¼Œåœ¨ä½¿ç”¨ä¸€ä¸ªå¤–éƒ¨æ‰“åˆ†å™¨ï¼ˆä½¿ç”¨ç¨ å¯†æ£€ç´¢çš„ç›¸ä¼¼åº¦è®¡ç®—ï¼‰ æ¥å¯¹ç”Ÿæˆæ¨¡å‹å¾—åˆ°çš„å€™é€‰åˆ—è¡¨è¿›è¡Œé‡æ–°æ’å*  |
| **Re-ranking / Scoring** | **scoring+ranking** |  **MQL4GRec** <br> <br> *æ¨¡å‹åŒæ—¶ä¼šè¾“å‡ºåŸºäºå›¾åƒä¸æ–‡æœ¬çš„åå¥½æ¨èåˆ—è¡¨ï¼Œå°†ç‰©å“åœ¨ä¸åŒåˆ—è¡¨çš„åˆ†æ•°ç›¸åŠ ï¼ˆå¦‚æœè¯¥ç‰©å“åªåœ¨ä¸€ä¸ªåˆ—è¡¨åˆ™åªç®—è¿™ä¸€ä¸ªåˆ—è¡¨ä¸­çš„å¾—åˆ†ï¼‰ä½œä¸ºæœ€ç»ˆå¾—åˆ†ï¼Œå†å¯¹ä¸¤ä¸ªåˆ—è¡¨çš„å†…å®¹è¿›è¡Œé‡æ’ï¼Œå–top-k*  |

---
