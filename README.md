# Awesome Modular Semantic-based Generative Recommendation

> A curated list of resources based on the **Five-Stage Modular Analysis Framework**:
> **Representation** $\rightarrow$ **Tokenization** $\rightarrow$ **Generative Backbone** $\rightarrow$ **Training Paradigm** $\rightarrow$ **Inference**.

## ğŸ“– Taxonomy Overview

This repository organizes Generative Recommendation (GenRec) research not by listing papers chronologically, but by dissecting them into the **modular pipeline**. This perspective reveals how different methods innovate at specific stages of the generation process.

---

## 1. Representation Layer
*Capture the input semantics before discrete quantization.*

| Strategy | Description | Representative Papers |
| :--- | :--- | :--- |
| **Semantic Embedding** | Utilizing PLMs (BERT/ViT) to extract textual or visual features. | **TIGER** (NeurIPS'23), **LETTER** (arXiv'24) |
| **Collaborative / Graph** | Fusing interaction signals (CF) into the semantic space. | **EAGER** (arXiv'23), **LC-Rec** (WWW'24) |
| **Multimodal Unified** | Jointly modeling text, image, and ID features. | **RPG** (KDD'25), **VGA** (ACL'24) |
| **Continuous Interaction** | Directly using raw interaction vectors (for Diffusion models). | **DiffRec** (SIGIR'23), **DDRM** (SIGIR'24) |

---

## 2. Tokenization Layer: Discretization for Generation
*å°†è¿ç»­è¡¨å¾ç¦»æ•£åŒ–ä¸ºå¯ç”Ÿæˆ Token (Codebooks)ã€‚*

| Tokenizer Family | Sub-Category | Paper's Tokenization Focus & Details |
| :--- | :--- | :--- |
| **Residual Quantization (RQ)** | **RQ-VAE** | **TIGER** (NeurIPS'23) <br> <img src="./assets/Tokenization-TIGER.png" width="600" /> <br> *åˆ©ç”¨å¤šå±‚æ®‹å·®é‡åŒ–å™¨å°† Item Embedding ç¼–ç ä¸ºå›ºå®šé•¿åº¦çš„ Token åºåˆ—ï¼Œé¦–æ¬¡å®ç° ID åˆ° Token çš„è½¬æ¢ã€‚* |
| |  | **LETTER** (CIKM'24) <br> <img src="./assets/Tokenization-LETTER.png" width="600" /> <br> *æå‡ºäº†å¯å­¦ä¹ çš„ Tokenizerï¼Œé€šè¿‡ **RQ-VAE** (è¯­ä¹‰æ­£åˆ™åŒ–)ã€**å¯¹æ¯”å¯¹é½æŸå¤±** (ååŒæ­£åˆ™åŒ–) å’Œ **å¤šæ ·æ€§æŸå¤±** å…±åŒä¼˜åŒ–ä»£ç æœ¬ï¼Œè§£å†³äº†ç°æœ‰ä»£ç æœ¬ç¼ºä¹ååŒä¿¡å·å’Œåˆ†é…åå·®çš„é—®é¢˜ã€‚* |
| | **R-KMeans** | OneRec  |
| **Product Quantization (PQ)** | **PQ** | RPG |
| | **OPQ** | **RPG** (KDD'25) <br> [RPGå¹¶è¡Œç”Ÿæˆ/OPQç¤ºæ„å›¾] <br> *è´¡çŒ®: é‡‡ç”¨äº†ç±»ä¼¼ OPQ çš„ç»“æ„æ¥æ„å»º Long Semantic IDï¼Œä»¥æ”¯æŒéè‡ªå›å½’çš„å¹¶è¡Œç”Ÿæˆï¼Œè§£å†³äº†çŸ­ ID è¯­ä¹‰å®¹é‡ä¸è¶³çš„é—®é¢˜ã€‚* |
| **Clustering-based** | **Hierarchical K-Means** | **GenRet** (NeurIPS'22) <br> [GenRetæ ‘ç»“æ„IDç”Ÿæˆå›¾] <br> *è´¡çŒ®: å°† Item ID ç¼–ç æˆæ ‘è·¯å¾„åºåˆ—ï¼Œåˆ©ç”¨å±‚æ¬¡ K-Means èšç±»æ„å»ºæ ‘ç»“æ„ï¼Œå°†ç”Ÿæˆé—®é¢˜è½¬åŒ–ä¸ºè·¯å¾„ç”Ÿæˆã€‚* |
| | **Hierarchical K-Means** | **SEER** (RecSys'23) <br> [SEER IDè§£é‡Šæ€§ç»“æ„å›¾] <br> *è´¡çŒ®: é‡‡ç”¨å±‚æ¬¡åŒ–çš„ç¦»æ•£ ID ç»“æ„ï¼Œåˆ©ç”¨èšç±»ç»“æœæ¥æä¾›æ¨èçš„å¯è§£é‡Šæ€§ï¼Œå¹¶æŒ‡å¯¼ç”Ÿæˆè¿‡ç¨‹ã€‚* |
| **Hybrid / Textual** | **Raw Text Tokens** | **GPT4Rec** (SIGIR eCom'23) <br> [GPT4Rec Query/Textç”Ÿæˆæµç¨‹å›¾] <br> *è´¡çŒ®: å°†ç”¨æˆ·å†å²è½¬åŒ–ä¸ºæ–‡æœ¬ Queryï¼Œç„¶åç”Ÿæˆ Item Title ç­‰æè¿°æ€§æ–‡æœ¬ï¼Œå®Œå…¨ç»•è¿‡äº† Item ID è¯­ä¹‰é‡åŒ–æ­¥éª¤ã€‚* |
| | **ID + Text Mixing** | **OneRec** (arXiv'25) <br> [OneRecæ··åˆTokenè¾“å…¥ç¤ºæ„å›¾] <br> *è´¡çŒ®: å°†ç¦»æ•£çš„ Item ID Token å’Œè¿ç»­çš„æ–‡æœ¬ Token ä½œä¸º LLM çš„è¾“å…¥ï¼Œå®ç°äº†ç»Ÿä¸€çš„æ£€ç´¢ä¸æ’åºã€‚* |
| **Learnable / E2E** | **Joint Optimization** | **LC-Rec** (WWW'24) <br> [LC-Recè”åˆä¼˜åŒ–æ¡†å›¾] <br> *è´¡çŒ®: æå‡ºäº†å¯å­¦ä¹ çš„ä»£ç æœ¬ï¼Œè®©é‡åŒ–å™¨åœ¨æ¨èä»»åŠ¡ä¸­åŒæ­¥ä¼˜åŒ–ï¼Œä»¥é€‚é…ç”Ÿæˆå¼éª¨å¹²ã€‚* |
| | **Joint Optimization** | **ETEGRec** (CIKM'24) <br> [ETEGRecç«¯åˆ°ç«¯æ¶æ„å›¾] <br> *è´¡çŒ®: ä¾§é‡äºå®ç° Tokenizer ä¸ç”Ÿæˆæ¨¡å—çš„ç«¯åˆ°ç«¯å¯è®­ç»ƒæ€§ï¼Œå‡å°‘é‡åŒ–è¯¯å·®å¯¹æ¨èæ€§èƒ½çš„å½±å“ã€‚* |

---

## 3. Generative Backbone
*The architecture modeling the probability of the token sequence.*

| Architecture | Pros & Cons | Representative Papers |
| :--- | :--- | :--- |
| **Encoder-Decoder (T5/BART)** | Bi-directional context encoding; good for mapping History $\to$ Target. | **TIGER**, **P5**, **VQ-Rec** |
| **Decoder-Only (LLM/GPT)** | Strong reasoning & zero-shot ability; standard for LLM-based approaches. | **OneRec**, **GPT4Rec**, **SGL** |
| **Non-Autoregressive (NAR)** | Parallel generation; significantly faster inference but harder to train. | **RPG** (KDD'25) |
| **Diffusion (Denoising)** | Iterative noise removal; generates continuous vectors, not tokens. | **DiffRec**, **LDiffRec** |

---

## 4. Training Paradigm
*How the system is optimized and aligned.*

| Paradigm | Description | Representative Papers |
| :--- | :--- | :--- |
| **Two-Stage (Quantize $\to$ Train)** | Step 1: Train Codebook (VQ-VAE). Step 2: Train Generator (Seq2Seq). | **TIGER**, **VQ-Rec** |
| **Joint / End-to-End** | Optimizing quantization loss and generation loss simultaneously. | **LC-Rec**, **GeneRec** |
| **Pre-train & Fine-tune** | Standard LLM paradigm: Language Modeling pre-training $\to$ Rec fine-tuning. | **GPT4Rec**, **P5** |
| **Alignment (RLHF/DPO)** | Aligning generation with ranking metrics or user feedback (Reinforcement Learning). | **OneRec** (Preference Alignment), **TallRec** |

---

## 5. Inference & Decoding
*Strategies to generate valid items and rank them efficiently.*

| Strategy | Description | Representative Papers |
| :--- | :--- | :--- |
| **Constrained Beam Search** | Using a Prefix Tree (Trie) to force the generator to output valid Item IDs. | **TIGER**, **GenRet**, **LETTER** |
| **Standard Beam Search** | Generating top-K sequences based on probability (may hallucinate invalid IDs). | **GPT4Rec** |
| **Parallel / Graph Decoding** | Non-autoregressive decoding guided by graph constraints. | **RPG** |
| **Re-ranking / Scoring** | Using the generator to score candidates retrieved by another model. | **LLaRA**, **TALLRec** |

---